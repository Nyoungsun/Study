{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS, StandardScaler as SDS\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터\n",
      " (506, 13) (506,)\n",
      "split + scailing x 데이터\n",
      " (354, 13) (152, 13)\n",
      "split + scailing y 데이터\n",
      " (354,) (152,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()        \n",
    "x = dataset.data                \n",
    "y = dataset.target              \n",
    "\n",
    "# print(\"x: \", x, \"\\ny: \", y)\n",
    "print(\"원본 데이터\\n\",x.shape, y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=3333)\n",
    "\n",
    "# scaler = MMS()\n",
    "scaler = SDS()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# print(\"x_test: \", x_test, \"\\nx_trian: \", x_train)\n",
    "# print(\"y_test: \", y_test, \"\\ny_trian: \", y_train)\n",
    "print(\"split + scailing x 데이터\\n\", x_train.shape, x_test.shape)\n",
    "print(\"split + scailing y 데이터\\n\", y_train.shape, y_test.shape)\n",
    "\n",
    "# ---------- RNN 모델에 적용하기위해 3차원으로 변환 ----------- #\n",
    "x_train = x_train.reshape(-1, 13, 1)\n",
    "x_test = x_test.reshape(-1, 13, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape = (13, 1), return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.5)) # 과적합 방지\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3)) # 과적합 방지\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2)) # 과적합 방지\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# LSTM을 두 개의 layer에서 사용할 때에는 return_sequences 파라미터를 지정해야한다. (default = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 컴파일, 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "Epoch 2/256\n",
      "Epoch 3/256\n",
      "Epoch 4/256\n",
      "Epoch 5/256\n",
      "Epoch 6/256\n",
      "Epoch 7/256\n",
      "Epoch 8/256\n",
      "Epoch 9/256\n",
      "Epoch 10/256\n",
      "Epoch 11/256\n",
      "Epoch 12/256\n",
      "Epoch 13/256\n",
      "Epoch 14/256\n",
      "Epoch 15/256\n",
      "Epoch 16/256\n",
      "Epoch 17/256\n",
      "Epoch 18/256\n",
      "Epoch 19/256\n",
      "Epoch 20/256\n",
      "Epoch 21/256\n",
      "Epoch 22/256\n",
      "Epoch 23/256\n",
      "Epoch 24/256\n",
      "Epoch 25/256\n",
      "Epoch 26/256\n",
      "Epoch 27/256\n",
      "Epoch 28/256\n",
      "Epoch 29/256\n",
      "Epoch 30/256\n",
      "Epoch 31/256\n",
      "Epoch 32/256\n",
      "Epoch 33/256\n",
      "Epoch 34/256\n",
      "Epoch 35/256\n",
      "Epoch 36/256\n",
      "Epoch 37/256\n",
      "Epoch 38/256\n",
      "Epoch 39/256\n",
      "Epoch 40/256\n",
      "Epoch 41/256\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00041: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'mse', optimizer='adam')\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', patience=16, restore_best_weights=True, verbose=3) # loss - min, accuracy - max \n",
    "hist = model.fit(x_train, y_train, epochs=256, batch_size=16, validation_split=0.2, callbacks = [earlyStopping], verbose=3) # verbose: 함수 수행시 발생하는 상세한 정보들을 표준 출력으로 자세히 내보낼 것인지"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가, 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(mse):  19.062942504882812\n",
      "RMSE:  4.366113192982679\n",
      "R2:  0.7353560207210108\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(x_test, y_test, verbose=3)\n",
    "print('loss(mse): ', loss)\n",
    "x_test\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "# print('x_test:\\n', x_test)\n",
    "# print('y_predict:\\n', y_predict)\n",
    "\n",
    "# print(hist) # <keras.callbacks.History object at 0x000001ECB4986D00>\n",
    "# print(hist.history) # 딕셔너리(key, value) → loss의 변화값을 list로(value는 list로 저장된다.)  \n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print(\"RMSE: \", RMSE)\n",
    "\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print(\"R2: \", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
